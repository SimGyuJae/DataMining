{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnI7CJTfRIS9"
      },
      "source": [
        "# Data Mining 2024-1 과제 \\#1\n",
        "\n",
        "## 1. 목표 <a id=\"intro\"></a>\n",
        "\n",
        "1. 해당 jupyter notebook 내의 지시사항을 따라 미완성된 코드를 모두 작성하세요.\n",
        "\n",
        "2. 아래의 분류 모델을 사용하여 주어진 데이터셋에 대한 정확도(Accuracy)를 구하는 코드를 완성해야 합니다.\n",
        "\n",
        "    - Decision Tree\n",
        "    - Naive Bayes\n",
        "    - Random Forest\n",
        "    - Gradient Boost (LightGBM, XGBoost)\n",
        "    - Support Vector Machine (SVM)\n",
        "    - Multi Layer Perceptron (MLP)\n",
        "    - Bagging\n",
        "    - Boosting\n",
        "    - Voting (Random Forest, XGBoost, LightGBM, SVM, MLP)\n",
        "    - Your own method\n",
        "\n",
        "3. 자신만의 방법으로 구현 시, 본인이 알고 있는 여러 방법을 적용하여 가장 높은 Accuracy를 보일 수 있는 코드를 제출해야 합니다.\n",
        "\n",
        "## 2. 환경 설정\n",
        "\n",
        "아래의 코드는 Python >= 3.6 를 만족하는 환경과 호환됩니다.\n",
        "\n",
        "채점 시 오류가 발생하지 않도록 본인이 작성한 코드 또한 호환되어야 합니다.\n",
        "\n",
        "Virtual Environment를 사용하여 위 version을 만족하는 환경을 구축하는 것을 권장합니다.\n",
        "\n",
        "아래 명시된 package 외에 추가적인 package를 설치하여 사용하는 것은 금합니다.\n",
        "\n",
        "\n",
        "1. **[scikit-learn](https://scikit-learn.org/stable/index.html)**\n",
        "\n",
        "2. **[numpy](https://numpy.org/doc/stable/)**\n",
        "\n",
        "3. **[pandas](https://pandas.pydata.org/docs/)**\n",
        "\n",
        "4. **[lightgbm](https://lightgbm.readthedocs.io/en/stable/)**\n",
        "\n",
        "5. **[xgboost](https://xgboost.readthedocs.io/en/stable/)**\n",
        "\n",
        "과제를 수행하기 전 명시된 package를 모두 설치하세요.\n",
        "\n",
        "주어진 package는 되도록 최신 version을 설치하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH50b0-cRITB"
      },
      "source": [
        "## 3. 구현사항 - 데이터셋\n",
        "\n",
        "### 3-1 데이터셋 소개\n",
        "\n",
        "본 과제에서는 다음의 데이터셋을 사용할 것입니다: `Pima Indian Diabetes Dataset`\n",
        "\n",
        "* Pima Indian Diabetes Dataset\n",
        "\n",
        "    이 데이터셋에서는 768명의 여성들에 대한 8 가지의 의료 수치 항목을 바탕으로 당뇨병 여부를 예측해야 합니다. (Binary Classification)\n",
        "\n",
        "해당 데이터셋은 `csv` 포맷의 파일로 주어졌습니다.\n",
        "\n",
        "이를 `pandas`의 `DataFrame`으로 불러와 사용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgxXflCjRITC"
      },
      "source": [
        "### 3-2. Load Pima Indian Diabetes Dataset **(주석 내에 코드를 작성하세요)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyAkVS9RITD",
        "outputId": "f2b1355d-5c9c-40cf-b731-4e0615f11417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "..           ...      ...            ...            ...      ...   ...   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "..                        ...  ...      ...  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n",
            "\n",
            "[768 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"/content\"   #\"./data\"\n",
        "PID_CSV_FILENAME = \"diabetes.csv\" # PID(Pima Indian Diabetes)\n",
        "\n",
        "############### Put your code here ###############\n",
        "#sample_data 옆에 저장함.\n",
        "#엑셀에서 =COUNTIF() 를 이용한 사전 처리 과정에서 768개의 데이터중 Outcome에는 0은 500개, 1은 268개 있음.\n",
        "file_path = f\"{DATA_DIR}/{PID_CSV_FILENAME}\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df)\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVx9UuvaRITF"
      },
      "source": [
        "### 3-4. Pima Indian Diabetesa Dataset Preprocessing **(주석 내에 코드를 작성하세요)**\n",
        "\n",
        "이 과정에서는 데이터셋을 분석하여 필요하다면 적절히 가공하고,\n",
        "\n",
        "학습 및 평가를 진행하기 전, 데이터셋을 `train`과 `test` subset으로 나눌 것입니다.\n",
        "\n",
        "더 높은 정확도를 위해 필요하다면 본인만의 방법을 적용하여도 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HO3fdf8NRITF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa8f97d-acc1-4c99-81ee-05affd43ed32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      1\n",
            "1      0\n",
            "2      1\n",
            "3      0\n",
            "4      1\n",
            "      ..\n",
            "763    0\n",
            "764    0\n",
            "765    0\n",
            "766    1\n",
            "767    0\n",
            "Name: Outcome, Length: 768, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "############### Put your code here ###############\n",
        "#사전처리 결과에 따르면 0과 1의 비율이 대략 500 : 268 이므로 stratify옵션을 통해 원본의 대략 65%, 35% 비율과 근사한 데이터 집합을 만들겠다.\n",
        "X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age' ]]\n",
        "y = df['Outcome']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, train_size = 0.75, random_state = 1, shuffle = True, stratify = y)\n",
        "print(y)\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx38jT27RITG"
      },
      "source": [
        "### 4. 구현사항 - 분류모델 학습과 평가 **(주석 내에 코드를 작성하세요)**\n",
        "\n",
        "[목표](#Intro)에서 설명하였듯이, 본인만의 방법을 포함하여 총 11가지의 분류 모델을 사용합니다.\n",
        "\n",
        "아래 목록과 코드를 참고하여 분류모델을 `train` subset으로 학습하고 `test` subset에 대한 정확도를 출력하는 코드를 작성하세요.\n",
        "\n",
        "Voting Classifier는 앞서 선언한 `Random Forest`, `LightGBM`, `XGBoost`, `SVM`, `MLP` 5가지의 분류 모델을 포함해야 합니다.\n",
        "\n",
        "- Decision Tree\n",
        "- Naive Bayes (Gaussian)\n",
        "- Random Forest\n",
        "- LightGBM\n",
        "- XGBoost\n",
        "- Support Vector Machine (SVM)\n",
        "- Multi Layer Perceptron (MLP)\n",
        "- Bagging\n",
        "- Boosting (AdaBoost)\n",
        "- Voting (Random Forest, XGBoost, LightGBM, SVM, MLP)\n",
        "- Your own method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZCQK0WCRITG"
      },
      "source": [
        "### 4-1. Decision Tree\n",
        "\n",
        "scikit-learn에서 제공하는 `Decision Tree Classifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0pRvS41cRITG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c68aec-c9ad-4ce4-e27f-565c580fbe86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "깊이= 1 에 대한 train 정확도는 0.7430555555555556 test 정확도는 0.7135416666666666\n",
            "깊이= 2 에 대한 train 정확도는 0.78125 test 정확도는 0.7447916666666666\n",
            "깊이= 3 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "깊이= 4 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "깊이= 5 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "깊이= 6 에 대한 train 정확도는 0.8715277777777778 test 정확도는 0.7447916666666666\n",
            "깊이= 7 에 대한 train 정확도는 0.9097222222222222 test 정확도는 0.734375\n",
            "깊이= 8 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7552083333333334\n",
            "깊이= 9 에 대한 train 정확도는 0.9479166666666666 test 정확도는 0.7395833333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "for i in range(1,10):\n",
        "    model = DecisionTreeClassifier(max_depth= i, random_state = 1).fit(X_train,y_train)\n",
        "    print(\"깊이=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))\n",
        "\n",
        "#깊이=6 이상에서 overfitting이 나타나므로 최적의 깊이는 5이다.\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IQ1iSPVRITG"
      },
      "source": [
        "### 4-2. Naive Bayes\n",
        "\n",
        "scikit-learn에서 제공하는 `Gausian Naive Bayes`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XoXhHrRwRITH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44574e80-0f9a-417e-8c37-171ebf91d313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.7135416666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "############### Put your code here ###############\n",
        "from sklearn.metrics import accuracy_score\n",
        "model = GaussianNB()\n",
        "model.fit(X_train,y_train)\n",
        "predicted = model.predict(X_test)\n",
        "print(\"test 정확도는\",accuracy_score(y_test, predicted))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6e5f7CRITH"
      },
      "source": [
        "### 4-3. Random Forest\n",
        "\n",
        "scikit-learn에서 제공하는 `Random Forest Classifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yqokspb0RITH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26afecd-7311-4b9f-e95f-850e0b4c9803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.734375\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "model = RandomForestClassifier(n_estimators=5, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"test 정확도는\", model.score(X_test,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tVmy_kERITI"
      },
      "source": [
        "### 4-4. LightGBM\n",
        "\n",
        "lightgbm에서 제공하는 `LGBMClassifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Op5QTs8kRITI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874d3639-55b1-4ca2-a50f-626a873bf921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 201, number of negative: 375\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000118 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 642\n",
            "[LightGBM] [Info] Number of data points in the train set: 576, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.348958 -> initscore=-0.623621\n",
            "[LightGBM] [Info] Start training from score -0.623621\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "test 정확도는: 0.7395833333333334\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "lgbm = LGBMClassifier()\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터로 예측\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# 정확도 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"test 정확도는:\", accuracy)\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi6noxa8RITI"
      },
      "source": [
        "### 4-5. XGBoost\n",
        "\n",
        "xgboost에서 제공하는 `XGBClassifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "62jJ0KKtRITI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439c10da-45fd-4fad-f884-e8248ecec4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.7291666666666666\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"test 정확도는\", model.score(X_test,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj5tj9WeRITI"
      },
      "source": [
        "### 4-6. SVM(Support Vector Machine)\n",
        "\n",
        "scikit-learn에서 제공하는 `SVC`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KX3AoKLuRITI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9ef929-f61e-4297-f6c0-a7fef0694633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.7135416666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "############### Put your code here ###############\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"test 정확도는\", clf.score(X_test,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLR5ddfmRITJ"
      },
      "source": [
        "### 4-7. MLP(Multi-Layer Perceptron)\n",
        "\n",
        "scikit-learn에서 제공하는 `MLPClassifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OuFHMJkxRITJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb47250-459f-4986-bb9a-e38dc68c2840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.7083333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train1 = scaler.transform(X_train)\n",
        "X_test1 = scaler.transform(X_test)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=100)\n",
        "mlp.fit(X_train1, y_train)\n",
        "print(\"test 정확도는\", mlp.score(X_test1,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX1FQqRJRITJ"
      },
      "source": [
        "### 4-8. Bagging\n",
        "\n",
        "scikit-learn에서 제공하는 `Bagging Classifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bQ2hW5RTRITJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76c4926-aed7-4b33-e4ab-447a2687e04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.7239583333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "clf = BaggingClassifier(estimator=SVC(),n_estimators=10, random_state=0).fit(X_train, y_train)\n",
        "print(\"test 정확도는\", clf.score(X_test,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk7s6Mw9RITJ"
      },
      "source": [
        "### 4-9. Boosting\n",
        "\n",
        "scikit-learn에서 제공하는 `AdaBoost Classifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2zwWIhsERITJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462460f2-cbac-4aea-cae8-a62fbf8724a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test 정확도는 0.75\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"test 정확도는\", clf.score(X_test,y_test))\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXT8iDseRITK"
      },
      "source": [
        "### 4-10. Voting\n",
        "\n",
        "scikit-learn에서 제공하는 `Voting Classifier`를 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요.\n",
        "\n",
        "Voting Classifier는 `Random Forest`, `LightGBM`, `XGBoost`, `SVM`, `MLP` 총 5 가지의 분류 모델을 포함해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bAQRwscpRITK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba94e85-ce62-4143-c253-32531f3df89b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 201, number of negative: 375\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 642\n",
            "[LightGBM] [Info] Number of data points in the train set: 576, number of used features: 8\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.348958 -> initscore=-0.623621\n",
            "[LightGBM] [Info] Start training from score -0.623621\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Accuracy: 0.7239583333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "############### Put your code here ###############\n",
        "# 분류기 정의\n",
        "rf_clf = RandomForestClassifier()\n",
        "lgbm_clf = LGBMClassifier()\n",
        "xgb_clf = XGBClassifier()\n",
        "svm_clf = SVC(probability=True)  # Voting Classifier에서 확률을 사용하기 위해 probability=True 설정\n",
        "mlp_clf = MLPClassifier()\n",
        "\n",
        "# Voting Classifier 생성\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', rf_clf),\n",
        "    ('lgbm', lgbm_clf),\n",
        "    ('xgb', xgb_clf),\n",
        "    ('svm', svm_clf),\n",
        "    ('mlp', mlp_clf)\n",
        "], voting='hard')  # 'hard' voting은 각 분류기의 결과를 다수결로 결정\n",
        "\n",
        "# Voting Classifier 학습\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 테스트 데이터로 예측\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# 정확도 출력\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "##################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGXcxwCaRITK"
      },
      "source": [
        "### 4-11. Your own method\n",
        "\n",
        "자신만의 방법으로 구현된 분류 모델을 `Pima Indian Diabetes Dataset`의 `train` subset으로 학습시키고 `test` subset에 대한 정확도를 출력하세요.\n",
        "\n",
        "필요하다면 주어진 Dataset을 추가적으로 가공하여 가장 높은 정확도를 달성하는 코드를 작성하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **주어진 데이터셋에 가장 높은 정확도를 보인 model은 DecisionTreeClassifier였다.따라서 해당 model의 파라미터를 바꾸어가며 최적화시키도록 하겠다. 우선 가장 중요한 트리의 최적 max_depth 후보를 선별하겠다.**"
      ],
      "metadata": {
        "id": "Ozcb9AXfquxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,20):\n",
        "    model = DecisionTreeClassifier(max_depth= i, random_state = 1).fit(X_train,y_train)\n",
        "    print(\"깊이=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjlMpiMTpc33",
        "outputId": "25570b61-cade-4bc8-970b-48fc0d73055c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "깊이= 1 에 대한 train 정확도는 0.7430555555555556 test 정확도는 0.7135416666666666\n",
            "깊이= 2 에 대한 train 정확도는 0.78125 test 정확도는 0.7447916666666666\n",
            "깊이= 3 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "깊이= 4 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "깊이= 5 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "깊이= 6 에 대한 train 정확도는 0.8715277777777778 test 정확도는 0.7447916666666666\n",
            "깊이= 7 에 대한 train 정확도는 0.9097222222222222 test 정확도는 0.734375\n",
            "깊이= 8 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7552083333333334\n",
            "깊이= 9 에 대한 train 정확도는 0.9479166666666666 test 정확도는 0.7395833333333334\n",
            "깊이= 10 에 대한 train 정확도는 0.96875 test 정확도는 0.7239583333333334\n",
            "깊이= 11 에 대한 train 정확도는 0.984375 test 정확도는 0.7239583333333334\n",
            "깊이= 12 에 대한 train 정확도는 0.9947916666666666 test 정확도는 0.7291666666666666\n",
            "깊이= 13 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 14 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 15 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 16 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 17 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 18 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n",
            "깊이= 19 에 대한 train 정확도는 1.0 test 정확도는 0.7239583333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**깊이는 iteration을 반복할 수록 0.72395에 수렴해가므로 이 보다 큰 정확도를 갖는 시도들 중 max_depth= 3,4,5,8 만 선별하여 random_state를 바꾸어가며 평균 성능을 구하고, max_depth를 하나로 정하겠다.**"
      ],
      "metadata": {
        "id": "aVCkFZ2grgLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#max_depth = 3일때\n",
        "test_accuracy = []\n",
        "for i in range(1,10):\n",
        "    model = DecisionTreeClassifier(max_depth= 3, random_state = i).fit(X_train,y_train)\n",
        "    print(\"random_state=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))\n",
        "    test_accuracy.append(model.score(X_test, y_test))\n",
        "print(\"평균 정확도:\",np.mean(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXkoCwJDp7cT",
        "outputId": "6cde6ae9-d87f-4769-ed86-0ca944c1b9f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_state= 1 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 2 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 3 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 4 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 5 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 6 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 7 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 8 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "random_state= 9 에 대한 train 정확도는 0.7847222222222222 test 정확도는 0.7604166666666666\n",
            "평균 정확도: 0.7604166666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max_depth = 4일때\n",
        "test_accuracy = []\n",
        "for i in range(1,10):\n",
        "    model = DecisionTreeClassifier(max_depth= 4, random_state = i).fit(X_train,y_train)\n",
        "    print(\"random_state=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))\n",
        "    test_accuracy.append(model.score(X_test, y_test))\n",
        "print(\"평균 정확도:\",np.mean(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIKE0d_Zp_CX",
        "outputId": "2755aa78-4e2b-4b63-9250-2a275e9e5bfb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_state= 1 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 2 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 3 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 4 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 5 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 6 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 7 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 8 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "random_state= 9 에 대한 train 정확도는 0.8003472222222222 test 정확도는 0.7552083333333334\n",
            "평균 정확도: 0.7552083333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max_depth = 5일때\n",
        "test_accuracy = []\n",
        "for i in range(1,10):\n",
        "    model = DecisionTreeClassifier(max_depth= 5, random_state = i).fit(X_train,y_train)\n",
        "    print(\"random_state=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))\n",
        "    test_accuracy.append(model.score(X_test, y_test))\n",
        "print(\"평균 정확도:\",np.mean(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chtc2Fw5t1PF",
        "outputId": "f52d781c-e4d6-48dc-dc35-8aad184b9524"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_state= 1 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 2 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 3 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7604166666666666\n",
            "random_state= 4 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7604166666666666\n",
            "random_state= 5 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 6 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 7 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 8 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7760416666666666\n",
            "random_state= 9 에 대한 train 정확도는 0.8420138888888888 test 정확도는 0.7604166666666666\n",
            "평균 정확도: 0.7708333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max_depth = 8일때\n",
        "test_accuracy = []\n",
        "for i in range(1,10):\n",
        "    model = DecisionTreeClassifier(max_depth= 8, random_state = i).fit(X_train,y_train)\n",
        "    print(\"random_state=\",i,\"에 대한 train 정확도는\", model.score(X_train,y_train), \"test 정확도는\", model.score(X_test,y_test))\n",
        "    test_accuracy.append(model.score(X_test, y_test))\n",
        "print(\"평균 정확도:\",np.mean(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2X7_VuAt40a",
        "outputId": "c00ec34b-7448-4069-c37a-2a48dd89560e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_state= 1 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7552083333333334\n",
            "random_state= 2 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.734375\n",
            "random_state= 3 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7552083333333334\n",
            "random_state= 4 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7395833333333334\n",
            "random_state= 5 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7447916666666666\n",
            "random_state= 6 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.734375\n",
            "random_state= 7 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7395833333333334\n",
            "random_state= 8 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7552083333333334\n",
            "random_state= 9 에 대한 train 정확도는 0.9340277777777778 test 정확도는 0.7447916666666666\n",
            "평균 정확도: 0.7447916666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**따라서 최적의 max_depth는 5이며, random_state 10개로 측정한 분류의 평균 정확도는  0.7708333333333334 이다.**"
      ],
      "metadata": {
        "id": "1vQGZog_uLyz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}